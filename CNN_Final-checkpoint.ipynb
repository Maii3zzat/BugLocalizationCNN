{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b60cdfda",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BugReportPreprocessing' object has no attribute 'startpreprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LAPTOP~1\\AppData\\Local\\Temp/ipykernel_1408/1250156476.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mbgpre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBugReportPreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#X = X.map(lambda x: bgpre.startpreprocess(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mbgpre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mtestRatio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.15\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BugReportPreprocessing' object has no attribute 'startpreprocess'"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Embedding, Lambda, SimpleRNN, Dropout, Dense, Concatenate, Conv1D, Bidirectional, LSTM, \\\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D, BatchNormalization, Activation, Input, Embedding, MaxPooling1D, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "#from Preprocessing import load_cleanData, clean_data\n",
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers.merge import concatenate\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "import pandas as pd\n",
    "from keras import layers, models, optimizers, Model\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import pickle\n",
    "from BugReportPreprocessing import BugReportPreprocessing  \n",
    "\n",
    "data = pd.read_csv(\"AspectJ.csv\")\n",
    "X = data['description'].astype('str')\n",
    "Y = data['files']\n",
    "\n",
    "bgpre = BugReportPreprocessing(data)\n",
    "#X = X.map(lambda x: bgpre.startpreprocess(x))\n",
    "bgpre.startpreprocess(data)\n",
    "\n",
    "testRatio = int(0.15 * len(data))\n",
    "print(testRatio)\n",
    "testData = data[0:testRatio]\n",
    "data = data[testRatio:300000]\n",
    "\n",
    "# Get the maximum length of all emails in the trianing set\n",
    "#maxlen = max([len(doc.split()) for doc in X])\n",
    "#print(maxlen)\n",
    "\n",
    "# set parameters:\n",
    "batch_size = 32\n",
    "embedding_dims = 100\n",
    "epochs = 15\n",
    "maxlen = 65\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)\n",
    "\n",
    "tokenizer = text.Tokenizer(oov_token=True)\n",
    "\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "train_x = tokenizer.texts_to_sequences(train_x)\n",
    "test_x = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=maxlen, padding='post')\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=maxlen, padding='post')\n",
    "\n",
    "\n",
    "def createEmbeddingsMatrix(tokenizer):\n",
    "    # Create an embedding dictionary by splitting the word from its vector representation\n",
    "    embeddings_index = dict()\n",
    "    # Loops over all the words in the glove file and extracts each word seperated from its vector\n",
    "    f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    # Create an empty embedding matrix with size of number of words x vector dimension\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dims))\n",
    "    # Map each word in tokenizer dictionary to vectors in the embedding dictionary\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index > vocab_size - 1:\n",
    "            break\n",
    "        else:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embeddings = createEmbeddingsMatrix(tokenizer)\n",
    "\n",
    "def singleCNN(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        layers.Embedding(vocab_size, embedding_dims, input_length=max_length, weights=[embeddings], trainable=False))\n",
    "    # model.add(layers.SpatialDropout1D(0.25))\n",
    "    model.add(layers.Conv1D(filters=50, kernel_size=2, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    # model.add(layers.SpatialDropout1D(0.25))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # compile the network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # model diagram\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = singleCNN(vocab_size, maxlen)\n",
    "history =model.fit(train_x,train_y,batch_size=batch_size,\n",
    "                       epochs=epochs,verbose=1,\n",
    "                       shuffle=True,\n",
    "                       validation_data=(test_x, test_y))\n",
    "\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85473b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b52b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
